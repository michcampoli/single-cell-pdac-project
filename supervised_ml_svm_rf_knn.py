# -*- coding: utf-8 -*-
"""supervised_ML_SVM_RF_KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17eiqTCsQ55sRkJPgWMCYKddLtqYxAof6

# **main dataset (PAAD CRA) SVM RF KNN**
"""

# Imports   
import numpy as np   
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy
import sklearn
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score, confusion_matrix

# Mounting GoogleDrive where data is stored
from google.colab import drive
drive.mount('/gdrive', force_remount=True)

#filename = 'svm_linear_model_first_7pt.sav'
directory = '/gdrive/My Drive/BMEN4480_Final_Project/'

"""# **data import**"""

# Import data
# Use the below to import the data in chuncks. This will take about 20-25 minutes
# THIS IS THE BIG FILE
# from pandas import *
tp = pd.read_csv(r'/gdrive/My Drive/BMEN4480_Final_Project/BMEN4480_data_log_prescvi_cell_types.csv', iterator=True, chunksize=10000)
df = pd.concat(tp, ignore_index=True)

# save this now for SVM
features_names = df.columns[1:19001].to_numpy() #feature names

# Check file and counts per class
print(df.shape)
df.cell_type.value_counts()

# 3 CLASSES

# change classes to 0, 1, 2
tumor = {'tumor class 1': 1, 'tumor class 2': 2, 'normal':0}
df.cell_type = [tumor[item] for item in df.cell_type]

# 2 CLASSES

# change classes to 0, 1
tumor = {'tumor class 1': 1, 'tumor class 2': 1, 'normal':0}
df.cell_type = [tumor[item] for item in df.cell_type]

# Rename the first column Cell
df = df.rename(columns={"Unnamed: 0": "Cell"})

# create a new variable to identify patients (from the first two letters of the cell name)
df['Patient'] = df.Cell.str[:3]

# look at cells per patient to choose a smaller dataset to work with
df.Patient.value_counts()

# extract random sample from the big file to create a work file
# THIS IS THE SMALLER FILE TO BE USED TO TEST THE PROGRAM

# choose by patient, so can stratify by patient and avoid batch effects

small_df = df[df.Patient.isin(['T11','T2_','T19','T23','N1_','N9_','T9_','T12','T22'])] #more cells/patient, less patients. top list
# small_df = df[df.Patient.isin(['N3_','T20','T8_','N6','T7_','T21','T10','N5_','N4_','T4_','N7_','T5_','T1_','N8_','N8_','T3_','N11','N10','T18','T16'])] # bottom list

del df

small_df.cell_type.value_counts()

# create train and test samples stratified by patient
from sklearn.model_selection import train_test_split
small_df_train, small_df_test = train_test_split(small_df, test_size=0.2, stratify=small_df[["Patient"]])

del small_df

# Remove class, cell name, and patient name from X. just genes.
X_Test = small_df_test.drop(columns=['cell_type','Cell','Patient']).to_numpy()

Y_Test = small_df_test[['cell_type']]
Y_Test = Y_Test.to_numpy().flatten()

# create np.array for X
X_Train = small_df_train.drop(columns=['cell_type','Patient','Cell']).to_numpy()

# create np.array for Y
Y_Train = small_df_train[['cell_type']]
Y_Train = Y_Train.to_numpy().flatten()

del small_df_test, small_df_train

"""# **meta data-- not used**"""

# import meta data file
# data_meta=pd.read_csv('/gdrive/My Drive/BMEN4480_Final_Project/PAAD_CRA001160_CellMetainfo_table.tsv',sep='\t')
# data_meta.head(10)

# see cell types. (not used in final analysis)
# data_meta['Celltype (original)'].value_counts()

# data_meta.shape

"""# **Logistic Regression-- not used**

Can only do for binary classes. Not for multiclass.
"""

# # logistic regression (not used in final analysis because we switched from binary to multiclass)
# from sklearn.linear_model import LogisticRegressionCV
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import confusion_matrix

# Informative filtering-- pass each feature and all cells through logistic regression. 
# Look at score after each. See which features are most important (highest statistic)
# Drop genes with low predictive value. 
list_X = []
for i in range(X.shape[1]):
  X_i = X[:,:]
  X_i = X_i.reshape(-1,1)
  list_X.append(X_i)

print(list_X[0])

list_scores = []

for i in range(len(list_X)):
  clf = LogisticRegression(random_state=0).fit(list_X[i],y_binary)
  score = clf.score(list_X[i],y_binary)
  list_scores.append(score)

X1 = X[:,4].reshape(-1,1)
clf = LogisticRegression(random_state=0).fit(X1,y_binary)
score = clf.score(X1,y_binary)
print(score)

list_scores

Crosstab between healthy cells, tumor cells, healthy patient, tumor patient
(avals, xvals), count = crosstab(data['cell_type'], data['Source'])

avals

xvals

count

"""# **Random Forrest**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

# Run model
#clf_rf = RandomForestClassifier(max_depth=2, random_state=0)
clf_rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)
clf_rf.fit(X_Train, Y_Train)
predicted_rf = clf_rf.predict(X_Test)
print(clf_rf.predict(X_Test))

from sklearn.metrics import accuracy_score
print("Train set Accuracy: ", accuracy_score(Y_Train, clf_rf.predict(X_Train)))
print("Test set Accuracy: ", accuracy_score(Y_Test, predicted_rf))

print(classification_report(Y_Test,predicted_rf))
print(confusion_matrix(Y_Test,predicted_rf))

cm = confusion_matrix(Y_Test, predicted_rf)
sns.heatmap(cm, annot=True)
plt.title(('Confusion Matrix'))
plt.ylabel('Ground Truth')
plt.xlabel('Prediction')

# save model
import pickle
pickle.dump(clf_rf, open(directory+'rf_model_first_9pt.sav', 'wb'))

"""# **SVM**"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
# %matplotlib inline

from matplotlib.pyplot import figure

clf_svm = svm.SVC(kernel= 'linear')
clf_svm.fit(X_Train,Y_Train)

#features_names = df.columns[1:19001].to_numpy() #feature names

# show top #(num_features) important features for coef. set coef[0], coef[1], or coef[2] to see each class.
def f_importances(coef,names,num_features):
    sorted_idx = coef.argsort()
    names = features_names[sorted_idx]
    coef_s = coef[sorted_idx]
    np.save(directory+'SVM_features_2_class', names[-num_features:])
    plt.barh(names[-100:], coef_s[-100:])
    plt.xlabel("Feature Importance")
    plt.rcParams['figure.figsize'] = [26, 23]
    plt.savefig(directory+'SVM_coef_2_class.png', dpi= 100)
    plt.show()

    # uncomment for barplot

# get 4,700 for 3 class new dataset
# get 14,100 for 2 class
f_importances(clf_svm.coef_[0], features_names, 14100)

f_importances(clf_svm.coef_[1], features_names, 4700)

f_importances(clf_svm.coef_[2], features_names, 4700)

"""save most important features w/ cells as separate file"""

features = np.load(directory+'SVM_features_2_class.npy', allow_pickle=True)

df_features = df[features]

df_features.to_pickle(directory+'df_features_2_class_ALL.pkl')

"""back to model"""

# train SVm model
predicted=clf_svm.predict(X_Test)

from sklearn import metrics
#print("Train set Accuracy: ", metrics.accuracy_score(Y_Train, clf_svm.predict(X_Train)))
print("Test set Accuracy: ", metrics.accuracy_score(Y_Test, predicted))

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

print(classification_report(Y_Test,predicted))
print(confusion_matrix(Y_Test,predicted))

cm = confusion_matrix(Y_Test, predicted)
sns.heatmap(cm, annot=True)
plt.rcParams['figure.figsize'] = [3, 2]
plt.title(('Confusion Matrix'))
plt.ylabel('Ground Truth')
plt.xlabel('Prediction')

# save model
import pickle
pickle.dump(clf_svm, open(directory+'svm_linear_model_first_9pt.sav', 'wb'))

# load model
clf_svm = pickle.load(open(filename, 'rb'))

"""Retrain model with just most informative genes

## **KNN**
"""

from sklearn.neighbors import KNeighborsClassifier

k = 4
#Train Model and Predict 
neigh = KNeighborsClassifier(n_neighbors = k).fit(X_Train,Y_Train)
neigh

# yhat = knn_model.predict(X_Test)
yhat = knn_model.predict(small_df_test_X)
yhat[0:5]

from sklearn import metrics
#print("Train set Accuracy: ", metrics.accuracy_score(Y_Train, neigh.predict(X_Train)))
print("Test set Accuracy: ", metrics.accuracy_score(Y_Test, yhat))

Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))

for n in range(1,Ks):
    
    #Train Model and Predict 
    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_Train,Y_Train)
    yhat=neigh.predict(X_Test)
    mean_acc[n-1] = metrics.accuracy_score(Y_Test, yhat)
 
    std_acc[n-1]=np.std(yhat==Y_Test)/np.sqrt(yhat.shape[0])
    
mean_acc

plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color="green")
plt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Neighbors (K)')
plt.tight_layout()
plt.show()

print( "The best accuracy was with", mean_acc.max(), "with k=", mean_acc.argmax()+1)

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

print(classification_report(small_df_test_Y,yhat))
print(confusion_matrix(small_df_test_Y,yhat))

cm = confusion_matrix(Y_Test, yhat)
sns.heatmap(cm, annot=True)
plt.rcParams['figure.figsize'] = [3, 2]
plt.title(('Confusion Matrix'))
plt.ylabel('Ground Truth')
plt.xlabel('Prediction')

# save model
import pickle
pickle.dump(neigh, open(directory+'knn_model_first_9pt.sav', 'wb'))

# load model
# import pickle
# knn_model = pickle.load(open(directory+'knn_model_first_9pt.sav', 'rb'))

"""## **SVM rbg kernel-- not used**"""

rbf_svm = svm.SVC(kernel='rbf')
rbf_svm.fit(X_Train,Y_Train)

predicted = rbf_svm.predict(X_Test)

accuracy = accuracy_score(Y_Test, predicted)
accuracy

print(accuracy_score(Y_Test,predicted))
print(classification_report(Y_Test,predicted))
print(confusion_matrix(Y_Test,predicted))

cm = confusion_matrix(Y_Test, predicted)
sns.heatmap(cm, annot=True)
plt.title(('Confusion Matrix'))
plt.ylabel('Ground Truth')
plt.xlabel('Prediction')

"""# **svm.NuSVM -- not used**"""

from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import NuSVC
clf_nusvm = make_pipeline(StandardScaler(), NuSVC())
clf_nusvm.fit(X_Train, Y_Train)
Pipeline(steps=[('standardscaler', StandardScaler()), ('nusvc', NuSVC())])

predicted = clf_nusvm.predict(X_Test)

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_Test, predicted)
accuracy

print(accuracy_score(Y_Test,predicted))
print(classification_report(Y_Test,predicted))
print(confusion_matrix(Y_Test,predicted))

cm = confusion_matrix(Y_Test, predicted)
sns.heatmap(cm, annot=True)
plt.title(('Confusion Matrix'))
plt.ylabel('Ground Truth')
plt.xlabel('Prediction')



"""# **Test on other half of dataset**"""

del clf_rf
del cm
del predicted

# load model
import pickle
clf_svm = pickle.load(open(directory+'svm_linear_model_first_9pt.sav', 'rb'))

predicted=clf_knn.predict(small_df_test_X)

# make small_df dataset to test on using code above
from sklearn import metrics
print("Test set Accuracy: ", metrics.accuracy_score(small_df_test_Y, predicted))

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
print(classification_report(small_df_test_Y,predicted))

cm = confusion_matrix(small_df_test_Y, predicted)
sns.heatmap(cm, annot=True)
plt.rcParams['figure.figsize'] = [3, 2]
plt.title(('Confusion Matrix'))
plt.ylabel('Ground Truth')
plt.xlabel('Prediction')

"""# **Test new dataset-- get features and corresponding cells **"""

# concatenate feature for new dataset PAAD GSE
# 4,700 top features from each class
features0 = np.load(directory+'SVM_features_0.npy', allow_pickle=True)
features1 = np.load(directory+'SVM_features_1.npy', allow_pickle=True)
features2 = np.load(directory+'SVM_features_2.npy', allow_pickle=True)
features= np.concatenate((features0,features1,features2), axis=None)
np.save(directory+'features_concat',features)

features_concat = np.load(directory+'features_concat.npy', allow_pickle=True)

df_features = df[features_concat]

df_features = np.save(directory+'df_features_2_class',features)

# save features dataframe with correct size to retest models
df_features.to_pickle(directory+'df_features_2_class.pkl')